# Documentation Technique - D√©tails d'Impl√©mentation

Ce document d√©taille les briques logiques essentielles du projet **ATLAS-analytics**. Il isole les parties de code critiques pour expliquer le fonctionnement interne et justifier les choix d'architecture.

---

## 1. Pipeline de Nettoyage (`src/cleaning.py`)

Le nettoyage est la premi√®re √©tape critique. Des donn√©es bruyantes (URLs, emojis, fautes) d√©gradent la performance du LLM.

```python
def pipeline_cleaning(text: str) -> Dict[str, str]:
    """
    Pipeline complet de nettoyage
    """
    raw = safe_str(text)
    
    # 1. Extraction des emojis (pour analyse de sentiment potentielle)
    emojis = extract_emojis(raw)
    
    # 2. Nettoyage et Traduction
    # - Suppression des URLs et mentions (@user)
    # - D√©tection de la langue
    # - Traduction en Fran√ßais si n√©cessaire (via deep-translator)
    lang, t_fr_raw, t_clean = cleaning_with_translation(raw)
    
    return {
        "lang": lang,
        "text_translated_fr": t_fr_raw, # Texte brut traduit (pour le LLM)
        "text_clean": t_clean,          # Texte nettoy√© (pour affichage propre)
        "emojis": emojis
    }
```

### üîç Explication
Cette fonction orchestre plusieurs sous-t√¢ches :
1.  **S√©curisation** : `safe_str` g√®re les valeurs nulles ou non-string.
2.  **Extraction** : On garde les emojis de c√¥t√© car ils sont de forts marqueurs d'√©motion, m√™me si on les retire du texte principal pour simplifier la lecture.
3.  **Traduction** : Si un tweet est en anglais ou espagnol, il est traduit pour que le mod√®le (et l'analyste) travaille sur une base unifi√©e en fran√ßais.

### üí° Pourquoi ce choix ?
*   **S√©paration des concerns** : On s√©pare le texte "pour la machine" (traduit, brut) du texte "pour l'affichage" (nettoy√©, sans URLs).
*   **Robustesse** : G√©rer la langue en amont √©vite d'envoyer du bruit au LLM ou de lui demander de traduire, ce qui consommerait plus de tokens et diluerait son attention sur la classification.

---

## 2. Classification via LLM (`src/llm_classification.py`)

C'est le c≈ìur intelligent du syst√®me. Nous utilisons l'API Mistral pour classer les tweets.

```python
@retry(
    reraise=True,
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=30),
    retry=retry_if_exception_type((LLMClassificationError, Exception)),
)
def classify_single_tweet(client: Mistral, tweet_text: str) -> str:
    """
    Classifie un seul tweet avec m√©canisme de retry
    """
    try:
        # Construction du prompt avec le tweet
        prompt = create_prompt(tweet_text)
        
        # Appel API Mistral
        response = client.chat.complete(
            model="mistral-tiny", # Mod√®le optimis√© pour la vitesse/co√ªt
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1, # Tr√®s faible pour des r√©sultats constants
            max_tokens=200
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"Erreur classification tweet: {e}")
        raise LLMClassificationError(str(e))
```

### üîç Explication
*   **D√©corateur `@retry`** : Utilise la librairie `tenacity`. Si l'API √©choue (timeout, erreur 500), le code attend un peu (backoff exponentiel) et r√©essaie jusqu'√† 3 fois.
*   **Temp√©rature 0.1** : On veut de la classification, pas de la po√©sie. Une temp√©rature basse force le mod√®le √† √™tre d√©terministe et factuel.
*   **Mod√®le `mistral-tiny`** : Suffisant pour de la classification simple, beaucoup moins cher et plus rapide que les gros mod√®les.

### üí° Pourquoi ce choix ?
*   **Fiabilit√©** : Les appels r√©seaux sont instables par nature. Sans m√©canisme de retry, un batch de 1000 tweets planterait au moindre p√©pin r√©seau.
*   **Co√ªt/Perf** : Le choix du mod√®le et des param√®tres (max_tokens) est optimis√© pour traiter de gros volumes sans exploser le budget.

---

## 3. Parsing Robuste (`src/parse_llm_outputs.py`)

Les LLM ne renvoient pas toujours du JSON parfait. Ce module nettoie la sortie.

```python
def parse_llm_response(raw_response: Optional[str]) -> Dict[str, str]:
    """
    Parse une r√©ponse LLM de mani√®re s√©curis√©e avec fallback
    """
    try:
        # 1. Extraction chirurgicale du JSON
        # Cherche le pattern {...} m√™me s'il y a du texte autour
        json_str = extract_json_from_text(raw_response)
        
        if not json_str:
            return DEFAULT_VALUES.copy()
        
        parsed = json.loads(json_str)
        
        # 2. Normalisation stricte
        # V√©rifie que "motif" est bien dans notre liste pr√©d√©finie
        motif = normalize_value(parsed.get("motif"), VALID_MOTIFS, "Autre")
        
        # ... idem pour sentiment, urgence, churn ...
        
        return {
            "motif": motif,
            "sentiment": sentiment,
            # ...
        }
    except Exception:
        # 3. Fallback (Filet de s√©curit√©)
        return DEFAULT_VALUES.copy()
```

### üîç Explication
*   **Extraction Regex** : Le LLM peut dire "Voici le JSON : { ... }". `json.loads` √©chouerait sur la phrase compl√®te. La regex va chercher uniquement la partie `{...}`.
*   **Normalisation** : Si le LLM hallucine un motif "Probl√®me Wifi" alors que notre cat√©gorie est "Technique", ou √©crit "Positif" avec une majuscule, `normalize_value` corrige cela pour garder des donn√©es propres.

### üí° Pourquoi ce choix ?
*   **Qualit√© des donn√©es** : Pour faire des graphiques (camemberts, barres), il faut des cat√©gories exactes. On ne peut pas avoir "Technique", "technique" et "Pb technique" comme 3 cat√©gories diff√©rentes. Ce module garantit l'int√©grit√© des donn√©es.

---

## 4. Orchestration (`src/pipeline_enrichment.py`)

Ce script lie tout ensemble et g√®re le traitement par lots (batching).

```python
def enrich_with_llm(df, ...):
    # ...
    
    # Traitement par lots (Batching)
    # On envoie les tweets par paquets (ex: 10 par 10)
    batch_results = classify_dataframe_batch(client, texts, batch_size=LLM_BATCH_SIZE)
    
    # Parsing des r√©sultats
    parsed_results = parse_batch_responses(batch_results)
    
    # Fusion avec le DataFrame original
    for idx, parsed in zip(df.index, parsed_results):
        for col in llm_columns:
            df.loc[idx, col] = parsed[col]
            
    # Sauvegarde interm√©diaire (Checkpoint)
    if checkpoint_path:
        save_dataframe(df, checkpoint_path)
        
    return df
```

### üîç Explication
*   **Batching** : Au lieu d'appeler l'API tweet par tweet (trop lent) ou tout d'un coup (trop gros), on traite par petits groupes.
*   **Checkpointing** : Si le script plante apr√®s 5000 tweets sur 10000, on sauvegarde l'√©tat. Au prochain lancement, on ne reprend que les 5000 restants.

### üí° Pourquoi ce choix ?
*   **Performance** : Le batching permet de parall√©liser (c√¥t√© API) et de r√©duire l'overhead r√©seau.
*   **R√©silience** : Le checkpointing est indispensable pour les longs traitements (plusieurs heures). On ne veut pas tout recommencer √† z√©ro en cas de coupure.

---

## 5. Interface Frontend (`frontend/src/components/KPIGrid.tsx`)

Visualisation des donn√©es enrichies pour l'utilisateur final.

```tsx
export function KPIGrid({ data }: KPIGridProps) {
    // √âtat de chargement
    if (!data) return <div className="animate-pulse">Chargement...</div>;

    return (
        <div className="grid grid-cols-1 md:grid-cols-4 gap-4">
            {/* Carte KPI simple */}
            <KPICard 
                title="Total Tweets" 
                value={data.total_tweets.toLocaleString()} 
            />
            
            {/* Carte KPI avec indicateur couleur */}
            <KPICard
                title="Tweets N√©gatifs"
                value={data.negatifs.toLocaleString()}
                sub={`${data.negatifs_pct}% du total`}
                color="text-red-500" // Rouge pour alerter
            />
            
            {/* ... */}
        </div>
    );
}
```

### üîç Explication
*   **Composants R√©utilisables** : `KPICard` est d√©fini une fois et r√©utilis√© pour chaque m√©trique, assurant une coh√©rence visuelle.
*   **Typage TypeScript** : `KPIGridProps` d√©finit exactement quelles donn√©es sont attendues, √©vitant les bugs d'affichage si l'API change.

### üí° Pourquoi ce choix ?
*   **Exp√©rience Utilisateur** : L'interface doit √™tre imm√©diate. Les couleurs (rouge/vert) guident l'≈ìil vers l'information importante (n√©gatif/positif) sans effort cognitif.
